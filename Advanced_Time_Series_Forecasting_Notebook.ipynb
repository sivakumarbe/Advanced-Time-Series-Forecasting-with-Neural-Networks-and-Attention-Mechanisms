{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Advanced Time Series Forecasting with LSTM + Attention\n", "## Jupyter Notebook Version\n", "---\n", "This notebook contains:\n", "- Data preprocessing for forecasting\n", "- LSTM + Self-Attention model\n", "- Baseline LSTM model\n", "- Backtesting pipeline\n", "- Classification model (LightGBM + SHAP)\n", "\n", "\u26a0\ufe0f *Note: The full pipeline code is placed below in organized sections.*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udccc Import Libraries"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n", "import pandas as pd\n", "import tensorflow as tf\n", "from tensorflow.keras import layers, models\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import mean_absolute_error, mean_squared_error\n", "import optuna\n", "import lightgbm as lgb\n", "import shap\n", "import matplotlib.pyplot as plt"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udccc Self-Attention Layer Implementation"]}, {"cell_type": "code", "metadata": {}, "source": ["from tensorflow.keras import backend as K\n", "\n", "class SelfAttention(layers.Layer):\n", "    def __init__(self, return_sequences=False, **kwargs):\n", "        super(SelfAttention, self).__init__(**kwargs)\n", "        self.return_sequences = return_sequences\n", "\n", "    def build(self, input_shape):\n", "        self.W = self.add_weight(name='att_weight',\n", "                                 shape=(input_shape[-1], input_shape[-1]),\n", "                                 initializer='glorot_uniform', trainable=True)\n", "        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],),\n", "                                 initializer='zeros', trainable=True)\n", "        self.u = self.add_weight(name='att_u', shape=(input_shape[-1],),\n", "                                 initializer='glorot_uniform', trainable=True)\n", "\n", "    def call(self, inputs):\n", "        u_it = K.tanh(K.dot(inputs, self.W) + self.b)\n", "        scores = K.dot(u_it, self.u)\n", "        alphas = K.softmax(scores)\n", "        context = K.sum(inputs * K.expand_dims(alphas, -1), axis=1)\n", "        return context\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udccc Attention-LSTM Model"]}, {"cell_type": "code", "metadata": {}, "source": ["def build_attention_lstm(input_shape, units=64, dropout=0.2, lr=1e-3):\n", "    inputs = layers.Input(shape=input_shape)\n", "    x = layers.LSTM(units, return_sequences=True)(inputs)\n", "    x = layers.Dropout(dropout)(x)\n", "    x = SelfAttention()(x)\n", "    x = layers.Dense(units//2, activation='relu')(x)\n", "    outputs = layers.Dense(1)(x)\n", "    model = models.Model(inputs, outputs)\n", "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse', metrics=['mae'])\n", "    return model"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udccc Notebook Ready \u2013 Add Data Loading, Backtesting, and SHAP Sections Next"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}